{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca24b2cb",
   "metadata": {},
   "source": [
    "# Web Scraping Tutorial: Basketball Films from Wikipedia\n",
    "\n",
    "This tutorial is designed for undergraduate students, particularly those in the iSchool or affiliated with data-focused groups such as the Data Science Club at UIUC. It's intended for learners who are comfortable with Python fundamentals (loops, lists, dictionaries) and have completed at least one semester of Python instruction, but are entirely new to web scraping.\n",
    "\n",
    "Suppose you've ever worked on a data science or digital humanities project. In that case, you've likely encountered a frustrating challenge: the data you need exists, but it's trapped inside a webpage instead of a downloadable CSV. That's where web scraping comes in.\n",
    "\n",
    "In this hands-on workshop, you'll learn how to scrape and clean real-world data from Wikipedia, specifically, from a saved HTML copy of the [List of Basketball Films](https://en.wikipedia.org/wiki/List_of_basketball_films) article. The project will walk you through extracting movie metadata, removing footnote clutter, improving the dataset with additional information like directors and publishers, and exporting everything into a usable CSV file for further analysis.\n",
    "\n",
    "We'll use an offline HTML file rather than scraping Wikipedia live (which can trigger rate limits or vary over time). This ensures the process is repeatable and stable, while also respecting the source material.\n",
    "\n",
    "## What Will You Learn?\n",
    "\n",
    "By the end of this project, you should know how to:\n",
    "\n",
    "- Open and parse an HTML file using BeautifulSoup  \n",
    "- Locate and extract rows that are from the correct `<table>` tag  \n",
    "- Remove clutter such as footnotes [1] using regular expression (`re`)  \n",
    "- Extract and organize metadata:  \n",
    "  - Director, Producer, Writer, Cast, Production Company, Country, Budget, and Running Time  \n",
    "- Add additional columns to the dataset that are helpful:  \n",
    "  - `movie_link`: The film's Wiki URL from saved HTML copy  \n",
    "  - `footnote_text`: readable footnote content to show necessary references  \n",
    "\n",
    "Doing **ALL** of these updates will organize references and in-text citations and add important data from each movie into a well-structured format.\n",
    "\n",
    "## How to Retrieve the HTML File\n",
    "\n",
    "- **Location of HTML File**  \n",
    "  - Click this [Wikipedia link](https://en.wikipedia.org/wiki/List_of_basketball_films) to open the **List of basketball films** page. \n",
    "- **Instructions**  \n",
    "  - Right-click anywhere on the page and select **\"Save As\"** (or **\"Save Page As\"**) to download the full HTML file.  \n",
    "  - Save the file in the same folder as your Python script.\n",
    "\n",
    "## Libraries You'll Use\n",
    "\n",
    "First, if you haven't already, you must import essential tools for this tutorial. The tools we will be using today are:\n",
    "\n",
    "- **Import `os`**  \n",
    "  - This is not essential to the project; it will just be easier to handle file paths in your file directory.  \n",
    "  - If you devote this project to a specific folder, you can skip this installation entirely.  \n",
    "  - **Code to install:**\n",
    "    - Open the terminal and write the following command, and press enter. - Having trouble? [Link](https://www.geeksforgeeks.org/how-to-install-os-sys-module-in-python/) to detailed guide!\n",
    "      ```bash\n",
    "      pip install os\n",
    "      ```\n",
    "- **Import `re`**  \n",
    "  - Used for matching patterns using regular expressions (e.g., finding headers in text).  \n",
    "  - No installation needed, `re` is built into Python’s standard library.\n",
    "\n",
    "- **Import `time`**  \n",
    "  - Used to slow down scraping requests to avoid overloading servers (such as Wikipedia).  \n",
    "  - This is essential for ethical scraping and preventing IP bans.  \n",
    "  - **Warning:** If you skip this, you may overload the server and get IP banned.  \n",
    "  - No installation needed, `time` is built into Python’s standard library.\n",
    "\n",
    "- **Import `requests`**  \n",
    "  - Allows your script to fetch live HTML pages from Wikipedia.  \n",
    "  - Used in this tutorial to retrieve links to listed movies and film budgets.  \n",
    "  - **Code to install:** \n",
    "    - Open the terminal and write the following command, and press enter. - Having trouble? [Link](https://www.geeksforgeeks.org/how-to-install-requests-in-python-for-windows-linux-mac/) to detailed guide!\n",
    "      ```bash\n",
    "      pip install requests\n",
    "      ```\n",
    "- **From `bs4` import `BeautifulSoup`**  \n",
    "  - Parses raw HTML into structured data, enabling CSV export.  \n",
    "  - Also includes XML support, so no need for a separate XML import.  \n",
    "  - **Code to install:**\n",
    "    - Open the terminal and write the following command, and press enter. - Having trouble? [Link](https://www.geeksforgeeks.org/beautifulsoup-installation-python/) to detailed guide!\n",
    "      ```bash\n",
    "      pip install beautifulsoup4\n",
    "      ```\n",
    "  \n",
    "\n",
    "This tutorial will introduce you to web scraping and show you how to build structured and cleaned datasets from unstructured HTML files. This process is an essential skill in data science, digital journalism, research, and data management.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-Step Breakdown of the Scraping Process\n",
    "\n",
    "Let's begin building our scraper! Below is the first chunk of our program that parses the Wikipedia HTML, collects footnotes, and identifies the table of basketball films. I will walk you through each line and concept to enable you to create an amazing project. Let's go!\n",
    "\n",
    "### Step 1: Load the Saved HTML File from Wikipedia\n",
    "\n",
    "Use this code block structure to open the HTML file. This is similar to HTML and XML, but in this case, we are using the BeautifulSoup library, which utilizes the XML library. Make sure the file is correctly named and is exactly an HTML file and not XML. \n",
    "\n",
    "**Follow this code syntax:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9bd6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# STEP 1: Load the saved Wikipedia HTML file\n",
    "with open(\"List of basketball films - Wikipedia.html\", \"rb\") as file:\n",
    "    page = BeautifulSoup(file, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53c52b9",
   "metadata": {},
   "source": [
    "##### **What does this do?**\n",
    "- We open the `.html` file in **binary mode** (`rb`). This guarantees that **all** characters - including non-UTF-8 ones, are read properly.  \n",
    "- We parse the HTML file using BeautifulSoup with the `html.parser`.  \n",
    "- The result is stored in a BeautifulSoup object called `page`, which we'll use to navigate and piece together our CSV.\n",
    "\n",
    "**After this step**, we now have a fully structured HTML file from Wikipedia loaded into memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102118cd",
   "metadata": {},
   "source": [
    "### Step 2: Extract the Footnotes from the Bottom of the Page\n",
    "\n",
    "**Let's break this down:**  \n",
    "In this step, we want to extract the footnotes that correspond to the footnotes in each **Notes** column on the webpage.  \n",
    "First:\n",
    "- We create an empty dictionary called `footnotes`. This will store ID–text pairs from the file.  \n",
    "- We use CSS selectors via `page.select()` to grab all the `li` elements inside the `ol class=\"references\"` list. This is where Wikipedia stores its citations.  \n",
    "- `li[id]` makes sure we only grab the necessary items that actually have an ID, such as a footnote [1] in this case.  \n",
    "- Inside the loop we create, we store:  \n",
    "    - **Key**: This is the footnote's ID tag  \n",
    "    - **Value**: The text of the footnote itself, using `get_text(\" \", strip=True)`.  \n",
    "        - This is done to:  \n",
    "            - Flatten the nested tags into plain text  \n",
    "            - Strip the extra whitespace in the text  \n",
    "            - Separate the text with spaces so it is more readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32a0db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "footnotes = {}\n",
    "reference_list = page.select(\"ol.references > li[id]\")\n",
    "for ref in reference_list:\n",
    "    footnotes[ref[\"id\"]] = ref.get_text(\" \", strip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5a8eab",
   "metadata": {},
   "source": [
    "**Why do this?**\n",
    "\n",
    "- The footnotes in this specific Wiki file appear inline in the Notes column (e.g., [1]). Our goal is to **remove them from the main content in the Notes column** while also keeping them organized in a separate column or file, like `footnote_text`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e5948c",
   "metadata": {},
   "source": [
    "### Step 3: Find the Main Movie Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa518aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Find the table with all the movies listed\n",
    "table = page.find(\"table\", class_=\"wikitable sortable\")\n",
    "rows = table.find_all(\"tr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97467827",
   "metadata": {},
   "source": [
    "**What's going on here?**\n",
    "\n",
    "- First, we use `page.find()` to find the **table** that has the class `\"wikitable sortable\"`. This is the general format for most Wikipedia tables found on their website.  \n",
    "- This **table** contains **ALL** the basketball film data that we want:  \n",
    "    - Title, year, genre, and notes  \n",
    "- We can then use `find_all(\"tr\")` on the **table** to grab all of its rows (table rows).  \n",
    "\n",
    "**FYI:** The first row is usually the header (column name(s)), and the rest are the data within their respective rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3506ff33",
   "metadata": {},
   "source": [
    "### Step 4: Building Up the Full Scraper\n",
    "\n",
    "Now that we have all the table rows from the HTML file, we can move on to the next steps! The next task is to loop over the collected table rows, extract the data from each cell, clean the footnotes, create new data columns to hold upcoming information, and store the footnote text.\n",
    "\n",
    "**Here is how we do it step by step:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d857206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Set up the columns we want in our final CSV\n",
    "column_names = [th.get_text(strip=True) for th in rows[0].find_all(\"th\")]\n",
    "column_names += [\n",
    "    \"movie_link\", \"director\", \"producer\", \"writer\", \"cast\",\n",
    "    \"production_company\", \"country\", \"budget\", \"running_time\", \"footnote_text\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77933a1",
   "metadata": {},
   "source": [
    "**Why are we doing this?**  \n",
    "- The first row in the table (row 0) contains the column headers as specified earlier: **Title, Year, Genre**, and **Notes**.  \n",
    "- We use a list to extract and clean the text we want. This is important for referencing any text and identifying which references or citations were already present, allowing us to cross-check information.  \n",
    "    - Use `.get_text(strip=True)` to remove whitespace and unnecessary HTML tags.  \n",
    "- We then **extend** this list with additional column names that we will fill in later.  \n",
    "    - These include columns used in the `column_names` DataFrame.\n",
    "\n",
    "**Why is this all useful?**  \n",
    "- This helps make our final CSV structured and even more comprehensive, with new and more valuable information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846228a7",
   "metadata": {},
   "source": [
    "### Step 5: Looping Over the Table Rows and Extracting the Data\n",
    "\n",
    "Since this looping process is complex, I’ve split it into three manageable parts: Step 5a to Step 5c for your convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b589e788",
   "metadata": {},
   "source": [
    "#### Step 5a\n",
    "\n",
    "Before we can process each row in the movie table, we want to grab all the footnote citations that appear next to the text in the **Notes** column. They appear as numbers after the text, like this: [1].  \n",
    "- We must first initialize an empty list called `all_data`. This will hold all the cleaned and structured movie rows.  \n",
    "- Additionally, we want to create a base Wikipedia URL (`\"https://en.wikipedia.org\"`). This will help us later in the process to create appropriate full links to each individual movie page. This ensures that there won’t be any incomplete URLs and that the format remains clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceb8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "base_url = \"https://en.wikipedia.org\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78784b74",
   "metadata": {},
   "source": [
    "**Now we being the looping process through each movie row:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3a8cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Go through each movie row\n",
    "for row in rows[1:]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae6989f",
   "metadata": {},
   "source": [
    "- We then skip the first row because those are column headers \n",
    "- Inside each row, we can find all the `td` (table data) elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab9a537",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = row.find_all(\"td\")\n",
    "if not cells:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120896a1",
   "metadata": {},
   "source": [
    "- To make this loop perform better, it’s best to add a conditional that skips empty rows. If a row is empty (e.g., due to spacing or formatting), we skip it using `continue`.  \n",
    "\n",
    "#### Extracting the Footnotes  \n",
    "This is an essential step in our web scraping process. We need to collect the contents of any footnotes before they are removed from the HTML. This is **very** important to keep in this order because once a tag is removed using `.decompose()`, it is permanently deleted from the HTML tree. This means we can no longer access it or its contents.  \n",
    "- In wiki tables, footnotes appear as small numbers [1], [2], etc., inside the `sup` tags. These `sup` tags contain hyperlinks that direct users to the references section at the bottom of the page. This is why extracting the footnotes now is an important step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5cc0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5a: Extract footnote references before decomposing\n",
    "footnote_parts = []\n",
    "for sup in row.find_all(\"sup\"):\n",
    "    a_tag = sup.find(\"a\")\n",
    "    if a_tag and \"href\" in a_tag.attrs:\n",
    "        foot_id = a_tag[\"href\"].replace(\"#\", \"\")\n",
    "        if foot_id in footnotes:\n",
    "            footnote_parts.append(footnotes[foot_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7752b31b",
   "metadata": {},
   "source": [
    "- We loop through each `sup` tag in the row, since Wiki holds these footnotes as links/markers on their website.  \n",
    "- If the `sup` contains a hyperlink (`href`), we extract its footnote ID by removing the corresponding `#`.  \n",
    "- We then look up that ID in our **footnotes** dictionary (created in Step 2) and add the full footnote text to our `footnote_parts` list.  \n",
    "\n",
    "Finally, we combine the footnote parts into a single string. We separate them using a vertical bar (`|`), which is also referenced as an \"OR\" conditional in Python. We use this bar to keep the footnotes clean and readable within one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24e5149",
   "metadata": {},
   "outputs": [],
   "source": [
    "footnote_text = \" | \".join(footnote_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677def9b",
   "metadata": {},
   "source": [
    "**This string above** will be added later into our **final CSV** under the `footnote_text` column in Step 5c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e146f7f5",
   "metadata": {},
   "source": [
    "#### Step 5b: Clean the Visible Table Text by Removing Footnotes\n",
    "\n",
    "After we've successfully grabbed and stored the footnotes and their contents in Step 5a, we no longer need the visible footnote markers [1]. This reduces clutter, and the footnote no longer serves a purpose after being scraped.\n",
    "\n",
    "These footnote markers are usually embedded directly into the text within the `td` element and wrapped in the `sup` tag. If we do not remove these, they’ll be included in the final CSV file, which is not our intended goal.\n",
    "\n",
    "**Please add the following to your script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759aa0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sup in row.find_all(\"sup\"):\n",
    "    sup.decompose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dccbe0",
   "metadata": {},
   "source": [
    "The `row.find_all(\"sup\")` scans the entire HTML row for any remaining `sup` tags (footnote markers). The `sup.decompose()` method **permanently deletes** the tag ([1]) from the HTML tree, including **ALL** of its contents.\n",
    "- Think of it this way, we’ve already asked BeautifulSoup to grab the necessary content, the footnote text. Now that we no longer need the footnote tags themselves, we use `decompose()` to delete any remaining ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e80fd6",
   "metadata": {},
   "source": [
    "#### Step 5c: Extracting and Building the Final Cleaned Row\n",
    "\n",
    "Now that we have: \n",
    "- Collected Footnotes (Step 5a)\n",
    "- Removed the visible footnote tag/markers (Step 5b)\n",
    "\n",
    "We are ready to extract the **clean text** from the row and build a structured list of data. This list will represent one complete movie entry in the dataset. \n",
    "\n",
    "**Here is how to Extract the Clean Cell Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2b5d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_info = []\n",
    "for cell in cells:\n",
    "    row_info.append(cell.get_text(\" \", strip=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be60844",
   "metadata": {},
   "source": [
    "Our next steps:\n",
    "- We loop through each `td` cell in the row.  \n",
    "- Use `get_text(\" \", strip=True)` to extract only the text using:  \n",
    "    - `strip=True`: This removes all leading or trailing whitespace.  \n",
    "    - `\" \"`: This replaces any inner line breaks with a single space.  \n",
    "- Since we removed all `sup` markers/tags in Step 5b, this makes the entire process a lot easier, as the text is now **free of footnote markers**.  \n",
    "\n",
    "**Padding for Any Missing Cells**  \n",
    "- Some rows may have fewer cells than the expected number of columns if the `Notes` column field is blank.  \n",
    "- To avoid index errors and keep everything positioned correctly when writing into the CSV, we **pad** the row with empty strings `\" \"` until it has the correct number.  \n",
    "- **Important:** `-11` is used because we later add 11 more columns, as referenced in Step 4.  \n",
    "\n",
    "**Extract the Movie Link**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a29d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get movie Wikipedia link\n",
    "link = \"\"\n",
    "link_tag = cells[0].find(\"a\")\n",
    "if link_tag and \"href\" in link_tag.attrs:\n",
    "    link = base_url + link_tag[\"href\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d8f89b",
   "metadata": {},
   "source": [
    "**Process Explanation:**\n",
    "- We check if the first cell has a **hyperlink**. In this case, it contains the *movie title*.  \n",
    "- If yes, we create the **full Wikipedia link** by appending the `href` to the base URL.  \n",
    "- This lets us visit each individual Wiki page for additional scraping in Step 7.\n",
    "\n",
    "**Add Placeholders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fc9460",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_info += [link, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", footnote_text]\n",
    "all_data.append(row_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fc9bcd",
   "metadata": {},
   "source": [
    "- We add placeholders (`\"\"`) for the 8 fields that we will fill in later when scraping the individual movie pages:  \n",
    "    - `director, producer, writer, cast, company, country, budget, runtime`\n",
    "- We also add the cleaned `footnote_text` we gathered in earlier steps.  \n",
    "- Finally, we add this completed row to `all_data`, which is the final list that will be written into the **final CSV** file.  \n",
    "\n",
    "**Why This Matters:**  \n",
    "This step helps create a fully cleaned and structured version of the movie row with:  \n",
    "- Cleaned and properly positioned text  \n",
    "- A working Wiki link  \n",
    "- A place to hold the scraped values  \n",
    "- Any footnote references formatted to make the Notes column easier to read and understand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f5aced",
   "metadata": {},
   "source": [
    "### Step 6: Filter Out the Rows Without Wikipedia Links\n",
    "\n",
    "Now that we created a list that is both structured and cleaned, we need to do a little more cleanup. SOme rows might not include a workable Wikipedia link, and we want to remove any of those before we move on to the final webscraping in Step 7. \n",
    "\n",
    "**Code Explanation** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb57f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = [row for row in all_data if row[column_names.index(\"movie_link\")].startswith(\"http\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395424ce",
   "metadata": {},
   "source": [
    "This line uses a **list comprehension** - [What is List Comprehension?](https://www.w3schools.com/python/python_lists_comprehension.asp)  \n",
    "It lets you take an existing list and build a new one by applying some operation or change to each item, optionally filtering along the way.  \n",
    "\n",
    "Here is the process we will be doing:\n",
    "- Loop through every **row** in the `all_data` list.  \n",
    "- Verify whether the value in the `movie_link` column starts with an `http` path. This indicates that it is a valid link.  \n",
    "- Only **keep rows** that pass this verification process.\n",
    "\n",
    "#### **Why is this Necessary?**\n",
    "- Not every movie in the Wikipedia table has its own page. Some might be plain text without a hyperlink, or the `<a>` tag might be missing. This varies case by case depending on the HTML file used.  \n",
    "- If we try to scrape a Wikipedia page without a valid link, the request will fail and end, wasting the time it took to run the script entirely.  \n",
    "- By doing this check before Step 7 (web scraping), we can guarantee that:  \n",
    "    - Every remaining row we choose has a real and clickable Wiki link.  \n",
    "    - The script will only try to scrape actual pages, not blank or missing entries.\n",
    "\n",
    "**Code We Use**\n",
    "`column_names.index(\"movie_link\")`  \n",
    "We use this to find the index of the `movie_link` column so we can check each row's link field dynamically; meaning we don’t assume the link is always in a fixed position.  \n",
    "\n",
    "This matters because if your column order ever changes (e.g., someone moves the `movie_link` column to the end of the column list), a hardcoded index like `row[4]` will break or return an incorrect value.  \n",
    "\n",
    "By finding the position based on the column name, the code will always point to the correct link field, no matter where that column appears in the list.  \n",
    "\n",
    "**In short:** We are checking the link field dynamically so the code automatically adapts if the column order changes in the future.\n",
    "\n",
    "**End Result:**\n",
    "After this step: \n",
    "- `all_data` only contains valid, usable movie rows. \n",
    "- Each one has a working Wiki link for scraping in next step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202830ca",
   "metadata": {},
   "source": [
    "### Step 7: Scraping the Movie(s) Information\n",
    "\n",
    "Now that Step 6 is now complete, we can use those links to **visit each movie's page** and extract even more information. Perciely, we are scraping Wikipedia's infobox which is the table usually on the top right of each article. \n",
    "\n",
    "#### **The Function:** `scrape_movie_page(url)`\n",
    "This function we will use only takes a single input, a url. This will then take us to a Wikipedia page for each movie and hopefully return 8 values: \n",
    "- `director, producer, writer, cast, company, country, budget,` and `runtime`. \n",
    "\n",
    "If anything fails in this process, either it cannot find one of these 8 values within the page, the string just does not return the value for the coresponding row. \n",
    "\n",
    "#### **Part 1: Requesting the Web Page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58c35a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28af3fa5",
   "metadata": {},
   "source": [
    "This line of code:\n",
    "- Sends a request to the URL using `requests.get()`.\n",
    "- Sets a `User-Agent` header to mimic a real browser. This helps prevent Wikipedia from blocking the request.\n",
    "- Specifies a timeout with `timeout=10`, which ensures that the request doesn't hang indefinitely. You can extend this time if needed, but 10 seconds is a reasonable limit to assume the page isn't responding.\n",
    "\n",
    "#### **Part 2: Parsing the Page with BeautifulSoup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6baad5",
   "metadata": {},
   "source": [
    "This will convert the **raw** HTML of the page into a BeautifulSoup object, which makes it easy to search and extract information.\n",
    "\n",
    "#### **Part 3: Locate the InfoBox in the Webpage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f89895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = soup.find(\"table\", class_=\"infobox\")\n",
    "if not info:\n",
    "    return [\"\"] * 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02e8be9",
   "metadata": {},
   "source": [
    "Above: \n",
    "- Looks for the `<table>` with the classifer **\"infobox\"**, where the key information we need is located. (director, cast, ect.)\n",
    "- If there is **no infobox** the funciton will return a list of 8 empy strings, this will avoid crashing the script. \n",
    "\n",
    "#### **Part 4: Define the Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8472b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(label):\n",
    "    header = info.find(\"th\", string=re.compile(label, re.IGNORECASE))\n",
    "    if header:\n",
    "        cell = header.find_next_sibling(\"td\")\n",
    "        if cell:\n",
    "            return cell.get_text(\" \", strip=True)\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86671052",
   "metadata": {},
   "source": [
    "`get_text(label)`:\n",
    "- This **helper** searches the infobox on the webpage for the `<th>` (table header) that matches a specific label, this case it would be like: **\"Directed by\"**, ignoring case.\n",
    "- Then it grabs the `<td>` next to it\n",
    "- Using `.get_text(\" \", strip=True)` extracts clean text and joins it with spaces.\n",
    "- Then finally it returns an empty string if the label or value isn’t found.\n",
    "\n",
    "#### **Next**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7826fda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cast():\n",
    "    header = info.find(\"th\", string=re.compile(\"starring|cast\", re.IGNORECASE))\n",
    "    if header:\n",
    "        cell = header.find_next_sibling(\"td\")\n",
    "        if cell:\n",
    "            list_items = cell.find_all(\"li\")\n",
    "            if list_items:\n",
    "                return \", \".join([li.get_text(strip=True) for li in list_items])\n",
    "            return cell.get_text(\", \", strip=True)\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f924a9d",
   "metadata": {},
   "source": [
    "`get_cast()`: \n",
    "- Handles the **\"Starring\"** or **\"Cast\"** row, which may use either plain text or list items `(<li>)`.\n",
    "- If `<li>` tags exist, it joins them with commas.\n",
    "- Otherwise, it extracts the text from the `<td>` directly.\n",
    "- If nothing is found, it returns an empty string.\n",
    "\n",
    "#### **Part 5: Extract Values from the Infobox**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48792ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "director = get_text(\"Directed by\")\n",
    "producer = get_text(\"Produced by|Production\")\n",
    "writer = get_text(\"Written by\")\n",
    "cast = get_cast()\n",
    "company = get_text(\"Production company|Production companies|Studio\")\n",
    "country = get_text(\"Country\")\n",
    "budget = get_text(\"Budget\")\n",
    "runtime = get_text(\"Running time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c26913",
   "metadata": {},
   "source": [
    "- Each of these lines uses the `get_text()` or `get_cast()` function to find the correct field.\n",
    "- Notice some labels use multiple terms separated by **`|`**, which acts as a logical **OR operator** in regular expressions (not standard Python booleans). This allows the pattern to match **any one** of the given options.\n",
    "#### **Part 6: Return the Results** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a5ec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "return [director, producer, writer, cast, company, country, budget, runtime]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77e0eb6",
   "metadata": {},
   "source": [
    "- This function returns a list of all 8 values in the order they will be inserted into the CSV.\n",
    "\n",
    "#### **Part 7 - EXTRA: Error Handling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3f22ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "except Exception as e:\n",
    "    print(\"Error scraping:\", url)\n",
    "    return [\"\"] * 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fb3257",
   "metadata": {},
   "source": [
    "- This step is important becuase if something goes wrong (bad URL, timeout, missing tags), this try-except block ensures the script continues without crashing.\n",
    "- It prints which URL failed and safely returns 8 empty fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803e3424",
   "metadata": {},
   "source": [
    "### Step 8: Update Movie Rows with Scraped Data\n",
    "\n",
    "At this given point we have: \n",
    "- Collected each movie’s basic info from the Wikipedia table.\n",
    "- Saved the full link to each movie’s individual Wikipedia page.\n",
    "- Written a function (scrape_movie_page()) to visit that page and extract detailed information.\n",
    "\n",
    "**Finally** we now use `scrape_movie_page()` to loop through each row, extract the data from the individual movie page, and insert it into the right place in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceacdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in all_data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f87f8b",
   "metadata": {},
   "source": [
    "- We go row by row through `all_data`, which holds every movie and its basic info.\n",
    "\n",
    "- Each row is a list of values (e.g., `title, year, genre, notes, link,` and `empty placeholders`).\n",
    "\n",
    "#### **Get the Movie Link**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f3a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = row[column_names.index(\"movie_link\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831679af",
   "metadata": {},
   "source": [
    "- This will retieve the Wiki link for that specifc movie. \n",
    "- Use `column_names.index(\"movie_link\")` to find the correct position in the list **dynamically**, in case the column order ever changes.\n",
    "\n",
    "#### **We Print Progress**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb133e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scraping:\", url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dca1363",
   "metadata": {},
   "source": [
    "- This prints out the current movie link being scraped.\n",
    "\n",
    "- It’s helpful for monitoring our progress or debugging if a specific link causes an error. This is helpful because we can better understand when and where it went wrong. \n",
    "\n",
    "#### **Scrape the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf87c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "details = scrape_movie_page(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9826cbb",
   "metadata": {},
   "source": [
    "- Calls the scrape_movie_page() function (defined in Step 7)\n",
    "\n",
    "- Sends the current URL as input\n",
    "\n",
    "- Returns a list of 8 values we specified:\n",
    "    - `[director, producer, writer, cast, company, country, budget, runtime]`\n",
    "\n",
    "#### **Insert the Data into the Row**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc761dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "row[-10:-2] = details  # Fill in the 8 extra fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488d0640",
   "metadata": {},
   "source": [
    "**Breakdown**\n",
    "- `row[-10:-2]` targets the 8 empty fields we originally reserved in Step 5c.\n",
    "- The last 10 columns include: [movie_link, director, ..., runtime, footnote_text]\n",
    "- We want to fill everything except the link and the footnote (which is the last column, -1)\n",
    "- This line replaces those 8 empty fields with the new scraped data.\n",
    "\n",
    "#### **Pause Between Requests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27084de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28227b56",
   "metadata": {},
   "source": [
    "**Breakdown**\n",
    "- This adds a 1-second delay between each request.\n",
    "- It's a respectful and safe practice when scraping multiple pages, especially from public sites like Wikipedia.\n",
    "- It prevents your script from overloading the server or getting rate-limited or blocked.\n",
    "\n",
    "### Step 9: Write the Final Data to a CSV File\n",
    "\n",
    "At this point, we’ve:\n",
    "\n",
    "- Extracted all relevant movie info from the Wikipedia table,\n",
    "- Scraped extra details from each movie’s page (like director, cast, and budget), \n",
    "- Stored everything in a structured list called all_data.\n",
    "\n",
    "Now we write all of that cleaned, processed data to a CSV file so it can be used for analysis, sharing, or importing into a spreadsheet or database.\n",
    "\n",
    "##### **Open a CSV File for Writing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13431f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"basketball_films.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975dc0d8",
   "metadata": {},
   "source": [
    "- This opens (or creates) a file named `basketball_films.csv` in **write mode** `(\"w\")`.\n",
    "- `newline=\"\"` prevents blank rows from appearing between entries (a common issue on Windows).\n",
    "- `encoding=\"utf-8\"` verifies the file supports special characters (like accented letters).\n",
    "- `f` is the file object used within the with  the block and it's automatically closed when the block ends.\n",
    "\n",
    "#### **Create a CSV Writer, Write the Header Row & Write All Movie Rows**\n",
    "In this final step, we export our cleaned and enriched dataset to a `.csv` file for use in tools like Excel, Google Sheets, or pandas.\n",
    "\n",
    "We open the file using a `with` block to ensure it closes properly after writing. Then we create a CSV writer, write the column headers as the first row, and output all the movie data we’ve stored in `all_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9803f11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = csv.writer(f)\n",
    "writer.writerow(column_names)\n",
    "writer.writerows(all_data)\n",
    "print(\"Saved: basketball_films.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b63337",
   "metadata": {},
   "source": [
    "### Final Thoughts\n",
    "\n",
    "You’ve just completed a full data scraping and processing project using Python. Here’s a summary of what your code accomplished from start to finish:\n",
    "\n",
    "- Loaded a saved HTML file from Wikipedia that contains a table of basketball films.\n",
    "- Parsed and extracted the table rows and headers, identifying key fields like Title, Year, Genre, and Notes.\n",
    "- Collected footnote references [1] from the bottom of the page and stored them separately.\n",
    "- Removed the footnote markers from the visible text in the table so the data is clean.\n",
    "- Extracted and cleaned each movie’s row of information, including its title and notes, and attached a working Wikipedia link if it was available in original HTML file.\n",
    "- Skipped any rows that didn’t include a real link to a Wikipedia page.\n",
    "- Defined a custom function (`scrape_movie_page()`) to visit each movie’s Wikipedia page and scrape detailed information like director, cast, budget, and runtime from the infobox.\n",
    "- Filled in those extra details into the correct columns for each movie.\n",
    "- Wrote all the structured and cleaned data into a CSV file so it can be used for analysis, opened in Excel, or imported into a data science project.\n",
    "\n",
    "This project introduced key concepts like:\n",
    "- HTML parsing with BeautifulSoup\n",
    "- Using regular expressions to match patterns in text\n",
    "- Dynamic indexing to make your code flexible\n",
    "- Writing structured data to a CSV file\n",
    "- Adding delays between web requests to avoid overloading a server\n",
    "- Handling missing data and potential errors safely\n",
    "\n",
    "By doing this project, you’ve worked through a real-world example of taking messy web data and turning it into something usable and well-organized. This kind of skill is valuable in data science, research, and even journalism.\n",
    "\n",
    "#### **Next Steps:**\n",
    "If you want to go even further with this project, here are some ways you can build on what you’ve done:\n",
    "\n",
    "- **Clean the data even more**\n",
    "  - Right now, the `budget` column is mixed. Some values are written like “$1 million” while others say just “1 million.” You could write code to:\n",
    "    - Remove dollar signs (`$`) and the word “million,”\n",
    "    - Convert values to pure numbers (e.g., \"$1 million\" becomes `1000000`),\n",
    "    - And leave the field blank if the value is unknown.\n",
    "  - The `runtime` column also includes extra text like “167 mins.” You can clean this by:\n",
    "    - Removing the word “mins” or “minutes,”\n",
    "    - And converting the value to an integer, like `167`.\n",
    "- **Add new features**\n",
    "  - Scrape more information, like release dates, box office totals, or Rotten Tomatoes scores (if available on the Wikipedia page).\n",
    "- **Make the script reusable**\n",
    "  - Turn your code into a tool that could work on other Wikipedia film lists, like football movies.\n",
    "\n",
    "By doing any of these extensions to the project, you’ll get more experience with data cleaning, web scraping, and real-world problem solving, while making your project even stronger."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
